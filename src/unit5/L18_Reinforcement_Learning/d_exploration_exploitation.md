# Exploration vs Exploitation

**Exploitation**

Exploitation in RL context means taking optimal action with respect to the current knowledge about the environment.

More formally, exploitation aims to take an action a from state s such that $\hat{Q}(s,a)$ is maximized, where $\hat{Q}$ is the current estimate of the $Q$-value function.

We collect samples and say that we are collecting estimations of the $Q$'s, but at each point that we update the $Q$'s, we are actually updating the policy.

So, instead of moving randomly, we are learning something from the policy, and acting based on what we already know.

Taken to the extreme, we put too much trust in prior knowledge, and never deviate too much from this - we may become stuck in a local optimum.

**Exploration**

Exploration goes to the other extreme, where we reject what happened in the past, and try new things each time as if there were no institutional memory.

**Epsilon greedy**

In reality, we combine these two approaches: at the beginning we have no knowledge, so explore more.

After a while, we select actions based on what we have learnt, and do exploitation.

This method is called epsilon greedy.

It changes epsilon as it goes, so epsilon tells us what the likelihood of taking an action randomly is.

At the beginning, we know nothing, so epsilon has a high value.

As $Q$-values start to converge, and to be more realistic, we want to mostly follow the $Q$'s, but still retain some random element so we are not fully committed to the existing policy.

$ε$ controls the exploration aspect of the RL algorithm: the higher the value of $ε$, higher are the chances that the agent takes a random action during the learning phase and higher are the chances that it explores new states and actions.

As the agent learns to act well, and has sufficiently explored its environment, $ε$ should be decayed off so that the value and $Q$ function samples get less noisy with some of the randomness in the agent's policy eliminated.
