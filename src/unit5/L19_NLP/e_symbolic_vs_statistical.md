# NLP - Symbolic vs Statistical Approaches

**Knowledge bottleneck in NLP**

In order to be able to interpret language, we need two things:

- knowledge about language
- knowledge about the world

It is the interaction of the two that helps us to interpret meaning.

Two broad approaches to the problem are the symbolic approach, and the statistical approach.

**Symbolic approach**

Encode all of the required information into a computer, such as grammar, relevant knowledge, and so on.

SHRDLU in the 1970s and 1980s was an example of the symbolic approach - it involved an elaborate manually-encoded knowledge representation to develop a machine that had specific domain applications.

The encoding was a major requirement, and no natural learning was involved. It would also only apply to a very narrow use-case.

**Statistical approach**

Infer language properties from language samples.

The statistical approach does not seek to develop understanding on the part of the machine, it is domain-specific and relevant to a particular task being trained for - spam detection, for example.

The 1990s saw the "Empirical Revolution", with the statistical approach rising to prominence.

PennTree Bank (1993), a database of one million words from the WSJ, manually annotated with syntactic structure, was instrumental in driving this change.
