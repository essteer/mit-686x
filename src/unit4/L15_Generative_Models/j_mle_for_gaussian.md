# MLE for Gaussian Distribution

P(x|μ, σ<sup>2</sup>) = 1 / (2πσ<sup>2</sup>)<sup>d/2</sup> exp<sup>(-(1/(2σ<sup>2</sup>))∥x - μ∥<sup>2</sup>)</sup>

If we have a training set, S, then we want to find the μ and σ<sup>2</sup> that will give the highest likelihood to our training data.

S<sub>n</sub> = { x<sup>(t)</sup> | t=1,⋯,n }

Since all of the points are independent, whenever we consider the likelihood that these points were generated by the specific Gaussians, we must multiply the likelihood of every point to be generated by the Gaussian.

P(S<sub>n</sub>|μ, σ<sup>2</sup>) = Π<sub>t=1</sub><sup>n</sup> P(x<sup>(t)</sup>|μ, σ<sup>2</sup>)

For the estimation of a single point x<sup>(t)</sup>, we want the μ and σ<sup>2</sup> that gives us the highest likelihood.

Just as with multinomials, we will take the log to simplify the expression.

= log Π<sub>t=1</sub><sup>n</sup> 1 / (2πσ<sup>2</sup>)<sup>d/2</sup> exp<sup>(-(1/(2σ<sup>2</sup>))∥x - μ∥<sup>2</sup>)</sup>

We know that the log of the product log Π is in fact the sum of logs:

= Σ<sub>t=1</sub><sup>n</sup> log 1 / (2πσ<sup>2</sup>)<sup>d/2</sup> + Σ<sub>t=1</sub><sup>n</sup> log exp<sup>(-(1/(2σ<sup>2</sup>))∥x - μ∥<sup>2</sup>)</sup>

Now compute the logs:

= Σ<sub>t=1</sub><sup>n</sup> - d/2 log (2πσ<sup>2</sup>) + Σ<sub>t=1</sub><sup>n</sup> - 1/(2σ<sup>2</sup>) ∥x - μ∥<sup>2</sup>

Since we are just summing n times, we can multiply by n instead of taking the sum:

= -(nd)/2 log (2πσ<sup>2</sup>) - 1/(2σ<sup>2</sup>) Σ<sub>t=1</sub><sup>n</sup> ∥x - μ∥<sup>2</sup>

Call this expression L:

L = -(nd)/2 log (2πσ<sup>2</sup>) - 1/(2σ<sup>2</sup>) Σ<sub>t=1</sub><sup>n</sup> ∥x - μ∥<sup>2</sup>

Now we want to find the best μ and σ<sup>2</sup>:

dL/dμ = 0

dL/dσ<sup>2</sup> = 0

The optimal μ will be the average of all the points, and the optimal σ<sup>2</sup> will be the average variance of the points from the μ.
