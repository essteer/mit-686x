# Prediction

Take a classification problem for linearly separable +ve and -ve points.

If we receive a new document, how do we know to which class it belongs?

We can look at the likelihood that the document was generated by the +ve class and the -ve class, and take the log of this:

$log \frac{P(D|θ^{+})}{P(D|θ^{-})}$

We want to assign the document to the class that gives it the highest likelihood.

For now, we make the simplifying assumption that the likelihood of being in either the +ve class or the -ve class is the same.

$log \frac{P(D|θ^{+})}{P(D|θ^{-})} = \lbrace \geq 0, +ve; \lt 0, -ve \rbrace$

These are known as class-conditional distributions.

Our expression can be rewritten as:

$log P(D|θ^{+}) - log P(D|θ^{-})$

and then as:

$log \prod_{w∈W} θ_w^{+} count(w) - log \prod_{w∈W} θ_w^{-} count(w)$

Recall that log of the product is the sum of logs.

$\sum_{w∈W} count(w) • logθ_w^{+} - \sum_{w∈W} count(w) • logθ_w^{-} = \sum_{w∈W} count(w) • log(θ_w^{+}/θ_w^{-})$

Here we introduce new notation as a stand-in for $log \frac{θ_w^{+}}{θ_w^{-}}$, as follows:

$\hat{θ}_{\hat{w}} = log \frac{θ_w^{+}}{θ_w^{-}}$

Our expression then becomes:

$\sum_{w∈W} count(w) • θ_{\hat{w}}^{\prime}$

This final form resembles the linear classifier, but is applicable to our generative model.
