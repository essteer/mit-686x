# Gaussian Generative Models

It is appropriate to use Gaussians, when we are looking at vectors in $ℝ^d$, where $d$ is the dimensionality of the space:

$x ∈ ℝ^d$

We are looking at a circular (or spherical) cloud of points, and will describe these points via two parameters.

The first parameter is the centre of the cloud, $\mu$.

The second parameter is a measure of how dispersed the cloud is, $\sigma^2$.

Thinking visually, the centre of the circle is $\mu$, and the radius of the cloud is $\sigma$.

The further away from the $\mu$, the lower the likelihood that the point was generated by this Gaussian.

So, $\mu$ is an average of all of the points, and \sigma^2 is just the average of the square of distances of $x$'s from $\mu$, the centre.

$P(x|\mu, \sigma^2) = \frac{1}{(2 \pi \sigma^2)^{d \over 2}} exp^{-(\frac{1}{2\sigma^2})∥x - \mu∥^2}$

Where: $\frac{1}{(2 \pi \sigma^2)^{d \over 2}}$ is the normalisation constant.

This is the likelihood of a particular point being generated by the Gaussian.

Clearly, if selecting different $\mu$ and $\sigma^2$, the same point may get different likelihoods.

**Note**

Of course, it is possible that the "clouds" may not form a circle, and there may be multiple clouds, but for the use of Gaussian generative models we assume that it is possible to take a measure of their mean and dispersion.

Generative models always involve assumptions, and it is a task for the researcher to decide which class of models is appropriate for their data.

**Multivariate Gaussian Random Vector**

A random vector $X = (X^{(1)},⋯,X^{(d)})X^T$ is a Gaussian vector, or multivariate Gaussian or normal variable, if any linear combination of its components is a (univariate) Gaussian variable or a constant (a “Gaussian" variable with zero variance), i.e., if $\alpha^TX$ is (univariate) Gaussian or constant for any constant non-zero vector $\alpha ∈ ℝ^d$.

The distribution of $X$, the $d$-dimensional Gaussian or normal distribution, is completely specified by the vector mean $\mu = E[X] = (E[X^{(1)}],⋯,E[X^{(d)}])^T$ and the $d \times d$ covariance matrix $\sigma$.

If $\sigma$ is invertible, then the $pdf$ of $X$ is:

$f_X(X) = \frac{1}{\sqrt{(2 \pi )^d det(\sigma)}} \exp^{- \frac{1}{2} (x-\mu)^T \sigma^{-1(x-\mu), x ∈ ℝ^d}}$

where $det(\sigma)$ is the determinant of the $\sigma$, which is positive when $\sigma$ is invertible.

If $\mu = 0$ and $\sigma$ is the identity matrix, then $X$ is called a standard normal random vector.

Note that when the covariant matrix $\sigma$ is diagonal, the $pdf$ factors into $pdfs$ of univariate Gaussians, and hence the components are independent.
