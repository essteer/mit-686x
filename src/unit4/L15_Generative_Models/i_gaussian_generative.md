# Gaussian Generative Models

It is appropriate to use Gaussians, when we are looking at vectors in ℝ<sup>d</sup>, where d is the dimensionality of the space:

x ∈ ℝ<sup>d</sup>

We are looking at a circular (or spherical) cloud of points, and will describe these points via two parameters.

The first parameter is the centre of the cloud, μ.

The second parameter is a measure of how dispersed the cloud is, σ<sup>2</sup>.

Thinking visually, the centre of the circle is μ, and the radius of the cloud is σ.

The further away from the μ, the lower the likelihood that the point was generated by this Gaussian.

So, μ is an average of all of the points, and σ<sup>2</sup> is just the average of the square of distances of x's from μ, the centre.

P(x|μ, σ<sup>2</sup>) = 1 / (2πσ<sup>2</sup>)<sup>d/2</sup> exp<sup>(-(1/(2σ<sup>2</sup>))∥x - μ∥<sup>2</sup>)</sup>

Where: 1 / (2πσ<sup>2</sup>)<sup>d/2</sup> is the normalisation constant.

This is the likelihood of a particular point being generated by the Gaussian.

Clearly, if selecting different μ and σ<sup>2</sup>, the same point may get different likelihoods.

**Note**

Of course, it is possible that the "clouds" may not form a circle, and there may be multiple clouds, but for the use of Gaussian generative models we assume that it is possible to take a measure of their mean and dispersion.

Generative models always involve assumtptions, and it is a task for the researcher to decide which class of models is appropriate for their data.

**Multivariate Gaussian Random Vector**

A random vector X = (X<sup>(1)</sup>,⋯,X<sup>(d)</sup>)X<sup>T</sup> is a Gaussian vector, or multivariate Gaussian or normal variable, if any linear combination of its components is a (univariate) Gaussian variable or a constant (a “Gaussian" variable with zero variance), i.e., if α<sup>T</sup>X is (univariate) Gaussian or constant for any constant non-zero vector α ∈ ℝ<sup>d</sup>.

The distribution of X, the d-dimensional Gaussian or normal distribution, is completely specified by the vector mean μ = E[X] = (E[X<sup>(1)</sup>],⋯,E[X<sup>(d)</sup>])<sup>T</sup> and the d x d covariance matrix Σ.

If Σ is invertible, then the pdf of X is:

f<sub>X</sub>(X) = 1/sqrt((2π)<sup>d</sup> det(Σ)) e<sup>- 1/2 (x-μ)<sup>T</sup>Σ<sup>-1</sup>(x-μ)</sup>, x ∈ ℝ<sup>d</sup>

where det(Σ) is the determinant of the Σ, which is positive when Σ is invertible.

If μ = 0 and Σ is the identity matrix, then X is called a standard normal random vector.

Note that when the covariant matrix Σ is diagonal, the pdf factors into pdfs of univariate Gaussians, and hence the components are independent.
