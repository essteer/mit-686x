# Likelihood of a Gaussian Mixture Model

Recall that our θ are given by:

θ: p<sub>1</sub>,⋯,p<sub>K</sub>, μ<sup>(1)</sup>,⋯,μ<sup>(K)</sup>, σ<sup>2</sup><sub>1</sub>,⋯,σ<sup>2</sup><sub>K</sub>

Before considering how to compute the θ, first consider how we can compute the likelihood of a particular point to be generated by these parameters.

The point can be generated from different clusters, so we look at the likelihood of it being generated from all possible clusters, then sum it up.

Going from 1 to K in this computation.

p(x|θ) = Σ<sub>j=1</sub><sup>K</sup> p<sub>j</sub> N(x, μ<sup>(j)</sup>, σ<sup>2</sup><sub>j</sub>)

The next question is, how we can find the different parameters given a specific assignment of points on the plane.

There is a lot of dependency, because in a general case we do not know from which cluster each of the points came.

**Likelihood**

We have a set of points S<sub>n</sub>:

p(S<sub>n</sub>|θ) = Π<sub>i=1</sub><sup>n</sup>Σ<sub>j=1</sub><sup>K</sup> p<sub>j</sub> N(x, μ<sup>(j)</sup>, σ<sup>2</sup><sub>j</sub>)

Given this expression, we then take the derivative with respect to our parameters, set it equal to zero and find the parameters.

This is a complex task, so we begin with an easy case, the "observed" case.

In the observed case, we receive the hard assignment, in that we are told that this point belongs to the first cluster, and that point belongs to the second cluster, and so on.

So we know, for example, which points are red and which are blue.

If we can solve that piece, we can translate it into the case where the assignment is not observed under the EM algorithm.
