# Mixture Model - Unobserved Case: EM Algorithm

The question that follows from the observed case is, how can we find the parameters with no information about the identities of the points?

The intuition behind how this can work is as follows.

In the observed case it was black and white, the point was either 0 or 1.

However, we know that the model actually provides probabilities for each of the points with respect to each of the clusters.

To deal with the fact that we don't know to which clusters our points belong, we break down the expression.

If we randomly guess the parameters - the p's, means, and variances for each of the components, this forms our first estimate.

(Think of K-means clustering, in which we randomly select the representatives of each cluster.)

Now we can compute the soft count - how likely it is that the point actually belongs to that cluster: how likely is it that point i belongs to cluster j.

So, instead of using δ, we can know use the probability and do exactly the same computations.

After the guesses have been made, we can compute the soft assignment of points to clusters.

The next natural step is that, now that the points belong to the clusters, we should go back and recompute the parameters.

I.e., maximising the likelihood of the observed data.

We can then again go and recompute the soft count, then again recompute the parameters, and continue to alternate until there is no change in parameters - convergence.

**EM algorithm**

Step 0 is to randomly initialise θ - all the means, variances, and probabilities: p<sub>1</sub>,⋯,p<sub>K</sub>, μ<sup>(1)</sup>,⋯,μ<sup>(K)</sup>, σ<sup>2</sup><sub>1</sub>,⋯,σ<sup>2</sup><sub>K</sub>.

**E Step**

Under this step, we go through each point and compute the likelihood that point i belongs to cluster j.

To make this assessment, we use posteriors.

p(j|i) = p<sub>j</sub> N(x<sup>(i)</sup>; μ<sup>(j)</sup>, σ<sup>2</sup><sub>j</sub>_I_) / p(x|θ)

We can compute the likelihood of x being generated by the mixture by taking the sum of all of the expressions along all of the mixture components.

This is the stage at which we can say how much red or blue the point has.

**M Step**

Now that we have all of the points, and know how red or blue they are, we want to re-estimate the parameters to make them more consistent with the current assignment of the soft counts.

So we want to re-estimate the p<sub>j</sub>, the μ<sup>(j)</sup>, and the σ<sup>2</sup><sub>j</sub>.

Instead of using the δ, we use probabilities, since those reflect our soft counts.

n^<sub>j</sub> is the size of the cluster, for which we take the soft counts - how much the point belongs to this cluster - and then sum them.

n^<sub>j</sub> = Σ<sub>i=1</sub><sup>n</sup> p(j|i)

We then need to compute the mixture weight of cluster j, for which we again take the size of the cluster (which is computed with soft counts) and divide it by the number of points.

p^<sub>j</sub> = n^<sub>j</sub> / n

Now, instead of an indicator function (because in this case each point belongs to every cluster, just with different probabilities), we just weight each point in the sum according to its likelihood to belong to that cluster.

μ^<sup>(j)</sup> = 1/n^<sub>j</sub> Σ<sub>i=1</sub><sup>n</sup> p(j|i) • x<sup>(i)</sup>

σ<sup>2</sup><sub>j</sub>^ = 1/n^<sub>j</sub>d Σ<sub>i=1</sub><sup>n</sup> p(j|i) • ∥x<sup>(i)</sup> - μ<sup>(j)</sup>∥<sup>2</sup>

Once we have completed the M-step, the result is that we have all the parameters after the first step.

We should now have a better estimate of the parameters, so we can go back and recompute the probability of points belonging to certain clusters, and continue until convergence.

**Note on EM algorithm**

EM algorithm is guaranteed to converge locally, but the point of convergence will depend on how it was initialised.

A major weak-point of the algorithm is therefore that the user needs to know how to initialise it.

To mitigate this, a reasonable initialisation could be found by running the K-means algorithm for the starting means, and perhaps use the global variance as the initial variance for each of the clusters, so that every cluster reaches all of the points.

In real-life applications, people typically look at a more simplified version of the problem to use it as the initialisation, and then run a more complex one.
