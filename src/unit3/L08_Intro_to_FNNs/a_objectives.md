# Objectives

**Unit**

At the end of this unit, you will be able to

- Implement a feedforward neural networks from scratch to perform image classification task.

- Write down the gradient of the loss function with respect to the weight parameters using back-propagation algorithm and use SGD to train neural networks.

- Understand that Recurrent Neural Networks (RNNs) and long short-term memory (LSTM) can be applied in modeling and generating sequences.

- Implement a Convolutional neural networks (CNNs) with machine learning packages.

**Lecture**

At the end of this lecture, you will be able to

- Recognize different layers in a feedforward neural network and the number of units in each layer.

- Write down common activation functions such as the hyperbolic tangent function , and the rectified linear function (ReLU).

- Compute the output of a simple neural network possibly with hidden layers given the weights and activation functions.

- Determine whether data after transformation by some layers is linearly separable, draw decision boundaries given by the weight vectors and use them to help understand the behavior of the network.
